# Deep Residual Learning for Image Recognition

* 작성자: YBIGTA 10기 김지중

* References
* [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
* [Coursera - Convolutional Neural Networks](https://www.coursera.org/learn/convolutional-neural-networks/home/welcome) WEEK 2


<br></br>
**1. 배경: degradation problem**

* 문제 현상
* 이론적으로 레이어를 쌓을 수록, 모델이 깊어질 수록 training error는 감소.
* 하지만 실제 구현단에서, 레이어를 단순히 쌓기만 하면(plain networks) 어느 순간부터 모델이 깊어질 수록 training error가 증가.
* 다시 강조하지만 training error의 증가를 이야기하고 있다. 오버피팅이 발생하여 test error가 증가한다는 것이 아니다.

<a href="https://imgur.com/S3coDg2"><img src="https://i.imgur.com/S3coDg2.png" title="source: imgur.com" /></a>

* 원인
* vanishing or exploding gradients

* 기존 해결방안
* initial normalization
* intermediate normalization (batch normalization)
* 이 논문에서는 아래와 같은 그래프를 얻기 위해 Deep Residual Learning를 제안

<a href="https://imgur.com/BXBL8c0"><img src="https://i.imgur.com/BXBL8c0.png" title="source: imgur.com" /></a>

<br></br>

**2. Residual Block**

ResNet의 근간이 되는 Residual Block을 살펴보자

<br></br>

2-1. Plain Network

<a href="https://imgur.com/OwXm7Ap"><img src="https://i.imgur.com/OwXm7Ap.png" title="source: imgur.com" /></a>

* <a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l]}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l]}" title="a^{[l]}" /></a> → Linear → ReLU → <a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l&plus;1]}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l&plus;1]}" title="a^{[l+1]}" /></a> → Linear → ReLU → <a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l&plus;2]}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l&plus;2]}" title="a^{[l+2]}" /></a>
* 수식으로 표현하면 아래와 같다.
* <a href="https://www.codecogs.com/eqnedit.php?latex=z^{[l&plus;1]}&space;=&space;W^{[l&plus;1]}a^{[l]}&plus;b^{[l&plus;1]}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?z^{[l&plus;1]}&space;=&space;W^{[l&plus;1]}a^{[l]}&plus;b^{[l&plus;1]}" title="z^{[l+1]} = W^{[l+1]}a^{[l]}+b^{[l+1]}" /></a>
* <a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l&plus;1]}&space;=&space;g(z^{[l&plus;1]})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l&plus;1]}&space;=&space;g(z^{[l&plus;1]})" title="a^{[l+1]} = g(z^{[l+1]})" /></a>
* <a href="https://www.codecogs.com/eqnedit.php?latex=z^{[l&plus;2]}&space;=&space;W^{[l&plus;2]}a^{[l&plus;1]}&plus;b^{[l&plus;2]}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?z^{[l&plus;2]}&space;=&space;W^{[l&plus;2]}a^{[l&plus;1]}&plus;b^{[l&plus;2]}" title="z^{[l+2]} = W^{[l+2]}a^{[l+1]}+b^{[l+2]}" /></a>
* <a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l&plus;2]}&space;=&space;g(z^{[l&plus;2]})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l&plus;2]}&space;=&space;g(z^{[l&plus;2]})" title="a^{[l+2]} = g(z^{[l+2]})" /></a>

<br></br>

2-2. Residual Block
* Plain Network의 흐름을 "main path"라고 부르자.
* Residual Block은 여기에 "short cut" 혹은 "skip connection"을 추가한 것이다.
* <a href="https://www.codecogs.com/eqnedit.php?latex=l&plus;2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?l&plus;2" title="l+2" /></a> 번째 ReLU를 거치기 전, <a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l]}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l]}" title="a^{[l]}" /></a>을 더해주는 것

<a href="https://imgur.com/22rVIQU"><img src="https://i.imgur.com/22rVIQU.png" title="source: imgur.com" /></a>

* 마지막 수식이 아래와 같이 변경된다.
* <a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l&plus;2]}&space;=&space;g(z^{[l&plus;2]}&plus;a^{[l]})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l&plus;2]}&space;=&space;g(z^{[l&plus;2]}&plus;a^{[l]})" title="a^{[l+2]} = g(z^{[l+2]}+a^{[l]})" /></a>

<br></br>

2-3. Residual Block 해석하기 - Identity Mapping

그래서 이게 뭐 어쨌다는 걸까? 아래 예시를 통해 살펴보자.

* 가정1: input(<a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l]}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l]}" title="a^{[l]}" /></a>)과 output(<a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l&plus;2]}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l&plus;2]}" title="a^{[l+2]}" /></a>)은 차원이 같다.
* 가정2: activation function(g)는 ReLU (x if x <a href="https://www.codecogs.com/eqnedit.php?latex=\ge" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\ge" title="\ge" /></a> x else 0)
* 가정3: <a href="https://www.codecogs.com/eqnedit.php?latex=W&space;=&space;0,&space;b&space;=&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?W&space;=&space;0,&space;b&space;=&space;0" title="W = 0, b = 0" /></a>
* <a href="https://www.codecogs.com/eqnedit.php?latex=L^2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L^2" title="L^2" /></a> 정규화 등의 weight decay를 통해 weight가 줄고 줄어 0에 수렴했다는 가정.
* 본인은 학습과정에서의 Worst Case에 대한 가정으로 이해함.

앞서 Residual Block에서 <a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l&plus;2]}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l&plus;2]}" title="a^{[l+2]}" /></a>의 수식은 아래와 같다고 밝혔다.
* <a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l&plus;2]}&space;=&space;g(z^{[l&plus;2]}&plus;a^{[l]})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l&plus;2]}&space;=&space;g(z^{[l&plus;2]}&plus;a^{[l]})" title="a^{[l+2]} = g(z^{[l+2]}+a^{[l]})" /></a>

⇔ <a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l&plus;2]}&space;=&space;g(W^{[l&plus;2]}&space;a{[l&plus;1]}&space;&plus;b^{[l&plus;2]}&space;&plus;&space;a^{[l]}))" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l&plus;2]}&space;=&space;g(W^{[l&plus;2]}&space;a{[l&plus;1]}&space;&plus;b^{[l&plus;2]}&space;&plus;&space;a^{[l]}))" title="a^{[l+2]} = g(W^{[l+2]} a{[l+1]} +b^{[l+2]} + a^{[l]}))" /></a>

⇔ <a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l&plus;2]}&space;=&space;g(a^{[l]})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l&plus;2]}&space;=&space;g(a^{[l]})" title="a^{[l+2]} = g(a^{[l]})" /></a> (∵ 가정3)

⇔ <a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l&plus;2]}&space;=&space;a^{[l]}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l&plus;2]}&space;=&space;a^{[l]}" title="a^{[l+2]} = a^{[l]}" /></a> (∵ 가정2에 따라 <a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l]}&space;\ge&space;0" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l]}&space;\ge&space;0" title="a^{[l]} \ge 0" /></a>)

<br></br>

**결론**
* Residual Block은 **최소한** input 그대로를 output으로 뱉는다.
* 따라서 레이어를 더 쌓는다고 학습에 해가 되지 않는다.
* 따라서 Plain Network와 달리, 레이어를 더 쌓는다고 training error가 오르는, degradation problem이 발생하지 않는다.

<br></br>

**3. Variations**

* 위 예제에서는 1개 레이어를 skip했다. 2~3개의 레이어를 skip 해도 효과가 있음을 실험을 통해 확인했다고 한다.
* 하지만 바로 전 레이어를 바로 더해주는 것은 효과가 없다고 한다.
* Residual Block의 input과 output의 dimension을 다르게 설계할 수도 있다.
* <a href="https://www.codecogs.com/eqnedit.php?latex=a^{[l&plus;2]}&space;=&space;g(z^[l&plus;2]&space;&plus;&space;W&space;a^{[l]})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?a^{[l&plus;2]}&space;=&space;g(z^[l&plus;2]&space;&plus;&space;W&space;a^{[l]})" title="a^{[l+2]} = g(z^[l+2] + W a^{[l]})" /></a> 와 같은 형태로, input에 가중치 매트릭스를 곱해 차원을 조절할 수 있다.
* 이 때, W를 모델의 파라미터로 둘 수도 있고(학습시킬 수도 있고), pre-train된 가중치를 가져와 쓸 수도 있고, 그냥 zero-padding 형식의 projection matrix로 둘 수도 있다고 한다.
* 어떤 방식이든 이러한 형태를 논문에서는 Projection Shortcuts라고 부른다. 혹자는 차원이 같은 Residual Block을 "Identity Block", Projection을 활용한 Residual Block을 "Convolutional Block"이라 부른다.

<br></br>

**4. Residual Network**

위에서 정의한 Residual Block을 쌓아 만든 네트워크가 Residual Network다.

예시 - ResNet-50

* Identity Block
<a href="https://imgur.com/PSU1pn7"><img src="https://i.imgur.com/PSU1pn7.png" title="source: imgur.com" /></a>

* Convolutional Block
<a href="https://imgur.com/HOlqylb"><img src="https://i.imgur.com/HOlqylb.png" title="source: imgur.com" /></a>

* Total Structure of ResNet -50
<a href="https://imgur.com/xJ7SNuL"><img src="https://i.imgur.com/xJ7SNuL.png" title="source: imgur.com" /></a>

<br></br>
끝.

